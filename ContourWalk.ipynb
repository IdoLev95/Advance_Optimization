{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JBRuZUkgF310"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to Show that on toy data set we can walk on the contour, reach abot L distance and still keeps the overall perforemence the same"
      ],
      "metadata": {
        "id": "b34fJR4lOA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "class ContourWalkingOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=required):\n",
        "        # Initialize the optimizer with the parameters and hyperparameters (e.g., learning rate)\n",
        "        defaults = dict(lr=lr)\n",
        "        super(ContourWalkingOptimizer, self).__init__(params, defaults)\n",
        "        self._is_first_time_calc = True\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        # Loop over parameter groups (usually one group, but could be more)\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Get gradient data for the parameter\n",
        "                grad = p.grad.data\n",
        "                # If this is the first call, initialize a random direction for p\n",
        "                if self._is_first_time_calc:\n",
        "                    d = torch.randn_like(grad)\n",
        "                    self.state.setdefault(p, {})['d'] = d\n",
        "                else:\n",
        "                    d = self.state[p]['d']\n",
        "\n",
        "                ## take here d from self\n",
        "                d_proj = self.__act(grad,d)\n",
        "                ## store here d_proj into self\n",
        "                self.state[p]['d'] = d_proj.clone()\n",
        "                # Custom update: Here, we perform a simple gradient descent step.\n",
        "                # This is equivalent to: p = p - lr * grad\n",
        "                p.data.add_(-group['lr'], d_proj)\n",
        "        if self._is_first_time_calc:\n",
        "            self._is_first_time_calc = False\n",
        "        return loss\n",
        "\n",
        "    def __act(self, grad, d):\n",
        "        grad_norm = torch.norm(grad)\n",
        "        if grad_norm < 1e-8:\n",
        "            return d  # or return d unchanged if gradient is nearly zero\n",
        "\n",
        "        # Compute dot product (flatten the tensors in case they are not 1D)\n",
        "        dot = torch.dot(grad.view(-1), d.view(-1))\n",
        "\n",
        "        # Project d onto the null space of grad\n",
        "        d_proj = d - (dot / (grad_norm ** 2)) * grad\n",
        "        norm_d_proj = torch.norm(d_proj)\n",
        "        if norm_d_proj < 1e-8:\n",
        "            return d  # or return d if the projection is negligible\n",
        "\n",
        "        d_proj = d_proj / norm_d_proj  # normalize the projected direction\n",
        "        return d_proj\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == '__main__':\n",
        "    # A simple linear model for demonstration\n",
        "    model = torch.nn.Linear(10, 1)\n",
        "    # Use Mean Squared Error as our loss function\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Instantiate our custom optimizer\n",
        "    L = 0.2\n",
        "    eta = 0.00001  # step size\n",
        "    T = int(L / eta)  # number of steps\n",
        "    optimizer_contour = ContourWalkingOptimizer(model.parameters(), lr=eta)\n",
        "    optimizer_score = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    num_samples = 100\n",
        "    # Dummy data: input x and target y\n",
        "    x = torch.randn(num_samples, 10)\n",
        "    y = torch.randn(num_samples, 1)\n",
        "\n",
        "    # Training loop (simplified)\n",
        "    for epoch in range(100):\n",
        "        optimizer_score.zero_grad()  # reset gradients to zero\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()  # compute gradients\n",
        "\n",
        "        optimizer_score.step()  # update parameters\n",
        "        print(f\"Epoch {epoch}: loss = {loss.item()}\")\n",
        "    optimized_params = [p.detach().clone() for p in model.parameters()]\n",
        "    for t in range(T):\n",
        "        optimizer_contour.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer_contour.step()\n",
        "        if t % 1000 == 0:\n",
        "            print(f\"Step {t}: loss = {loss.item()}\")\n",
        "    curr_params = model.parameters()\n",
        "    for i, (param_score, param_contour) in enumerate(zip(optimized_params, curr_params)):\n",
        "        print(f\"Parameter {i}: score = {torch.norm(param_score - param_contour)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrrNgqiUF4ZI",
        "outputId": "b27e00b3-fdc0-4777-aee7-469108b72521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 1.5175349712371826\n",
            "Epoch 1: loss = 1.4980511665344238\n",
            "Epoch 2: loss = 1.4795823097229004\n",
            "Epoch 3: loss = 1.4620710611343384\n",
            "Epoch 4: loss = 1.4454628229141235\n",
            "Epoch 5: loss = 1.4297064542770386\n",
            "Epoch 6: loss = 1.4147535562515259\n",
            "Epoch 7: loss = 1.4005591869354248\n",
            "Epoch 8: loss = 1.387081265449524\n",
            "Epoch 9: loss = 1.374279499053955\n",
            "Epoch 10: loss = 1.3621163368225098\n",
            "Epoch 11: loss = 1.3505568504333496\n",
            "Epoch 12: loss = 1.3395675420761108\n",
            "Epoch 13: loss = 1.3291174173355103\n",
            "Epoch 14: loss = 1.3191767930984497\n",
            "Epoch 15: loss = 1.3097187280654907\n",
            "Epoch 16: loss = 1.300716519355774\n",
            "Epoch 17: loss = 1.2921459674835205\n",
            "Epoch 18: loss = 1.2839840650558472\n",
            "Epoch 19: loss = 1.2762088775634766\n",
            "Epoch 20: loss = 1.268800139427185\n",
            "Epoch 21: loss = 1.2617384195327759\n",
            "Epoch 22: loss = 1.2550055980682373\n",
            "Epoch 23: loss = 1.248584508895874\n",
            "Epoch 24: loss = 1.2424590587615967\n",
            "Epoch 25: loss = 1.2366136312484741\n",
            "Epoch 26: loss = 1.231034278869629\n",
            "Epoch 27: loss = 1.2257072925567627\n",
            "Epoch 28: loss = 1.220619559288025\n",
            "Epoch 29: loss = 1.2157593965530396\n",
            "Epoch 30: loss = 1.2111150026321411\n",
            "Epoch 31: loss = 1.2066757678985596\n",
            "Epoch 32: loss = 1.2024314403533936\n",
            "Epoch 33: loss = 1.1983723640441895\n",
            "Epoch 34: loss = 1.1944894790649414\n",
            "Epoch 35: loss = 1.1907739639282227\n",
            "Epoch 36: loss = 1.1872179508209229\n",
            "Epoch 37: loss = 1.183813452720642\n",
            "Epoch 38: loss = 1.1805531978607178\n",
            "Epoch 39: loss = 1.177430272102356\n",
            "Epoch 40: loss = 1.1744381189346313\n",
            "Epoch 41: loss = 1.1715705394744873\n",
            "Epoch 42: loss = 1.1688220500946045\n",
            "Epoch 43: loss = 1.1661863327026367\n",
            "Epoch 44: loss = 1.1636587381362915\n",
            "Epoch 45: loss = 1.161233901977539\n",
            "Epoch 46: loss = 1.1589070558547974\n",
            "Epoch 47: loss = 1.1566739082336426\n",
            "Epoch 48: loss = 1.1545301675796509\n",
            "Epoch 49: loss = 1.152471661567688\n",
            "Epoch 50: loss = 1.1504945755004883\n",
            "Epoch 51: loss = 1.1485953330993652\n",
            "Epoch 52: loss = 1.1467703580856323\n",
            "Epoch 53: loss = 1.1450164318084717\n",
            "Epoch 54: loss = 1.1433302164077759\n",
            "Epoch 55: loss = 1.1417090892791748\n",
            "Epoch 56: loss = 1.140149712562561\n",
            "Epoch 57: loss = 1.1386499404907227\n",
            "Epoch 58: loss = 1.1372069120407104\n",
            "Epoch 59: loss = 1.1358181238174438\n",
            "Epoch 60: loss = 1.1344815492630005\n",
            "Epoch 61: loss = 1.1331946849822998\n",
            "Epoch 62: loss = 1.1319555044174194\n",
            "Epoch 63: loss = 1.130761981010437\n",
            "Epoch 64: loss = 1.1296123266220093\n",
            "Epoch 65: loss = 1.1285046339035034\n",
            "Epoch 66: loss = 1.1274371147155762\n",
            "Epoch 67: loss = 1.126408338546753\n",
            "Epoch 68: loss = 1.1254165172576904\n",
            "Epoch 69: loss = 1.1244601011276245\n",
            "Epoch 70: loss = 1.1235377788543701\n",
            "Epoch 71: loss = 1.1226481199264526\n",
            "Epoch 72: loss = 1.121789813041687\n",
            "Epoch 73: loss = 1.1209616661071777\n",
            "Epoch 74: loss = 1.1201624870300293\n",
            "Epoch 75: loss = 1.1193910837173462\n",
            "Epoch 76: loss = 1.118646264076233\n",
            "Epoch 77: loss = 1.1179273128509521\n",
            "Epoch 78: loss = 1.1172329187393188\n",
            "Epoch 79: loss = 1.1165622472763062\n",
            "Epoch 80: loss = 1.1159144639968872\n",
            "Epoch 81: loss = 1.115288496017456\n",
            "Epoch 82: loss = 1.1146835088729858\n",
            "Epoch 83: loss = 1.1140990257263184\n",
            "Epoch 84: loss = 1.1135340929031372\n",
            "Epoch 85: loss = 1.112987756729126\n",
            "Epoch 86: loss = 1.112459659576416\n",
            "Epoch 87: loss = 1.11194908618927\n",
            "Epoch 88: loss = 1.111454963684082\n",
            "Epoch 89: loss = 1.110977292060852\n",
            "Epoch 90: loss = 1.1105151176452637\n",
            "Epoch 91: loss = 1.1100680828094482\n",
            "Epoch 92: loss = 1.109635353088379\n",
            "Epoch 93: loss = 1.109216570854187\n",
            "Epoch 94: loss = 1.108811378479004\n",
            "Epoch 95: loss = 1.1084188222885132\n",
            "Epoch 96: loss = 1.108039140701294\n",
            "Epoch 97: loss = 1.1076713800430298\n",
            "Epoch 98: loss = 1.107315182685852\n",
            "Epoch 99: loss = 1.1069703102111816\n",
            "Step 0: loss = 1.1066361665725708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a6773b203df1>:43: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
            "  p.data.add_(-group['lr'], d_proj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000: loss = 1.1067050695419312\n",
            "Step 2000: loss = 1.1066062450408936\n",
            "Step 3000: loss = 1.10667085647583\n",
            "Step 4000: loss = 1.1066514253616333\n",
            "Step 5000: loss = 1.1068669557571411\n",
            "Step 6000: loss = 1.1069080829620361\n",
            "Step 7000: loss = 1.1070704460144043\n",
            "Step 8000: loss = 1.1071194410324097\n",
            "Step 9000: loss = 1.1071256399154663\n",
            "Step 10000: loss = 1.1070961952209473\n",
            "Step 11000: loss = 1.10708749294281\n",
            "Step 12000: loss = 1.1073414087295532\n",
            "Step 13000: loss = 1.1071281433105469\n",
            "Step 14000: loss = 1.1072735786437988\n",
            "Step 15000: loss = 1.1072250604629517\n",
            "Step 16000: loss = 1.1073870658874512\n",
            "Step 17000: loss = 1.1073914766311646\n",
            "Step 18000: loss = 1.1074209213256836\n",
            "Step 19000: loss = 1.1073129177093506\n",
            "Parameter 0: score = 0.17514707148075104\n",
            "Parameter 1: score = 0.007978945970535278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try on a real dataset"
      ],
      "metadata": {
        "id": "21x8xkZVOXUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split,DataLoader\n",
        "# Simple MLP for MNIST\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# MNIST dataset and dataloader\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training set into training and validation sets (e.g., 80/20 split)\n",
        "num_val = len(mnist_test)\n",
        "num_train = len(train_dataset) - num_val\n",
        "mnist_train, val_set = random_split(train_dataset, [num_train, num_val])\n",
        "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "val_loader   = DataLoader(val_set, batch_size=64, shuffle=True)\n",
        "val_loader_no_contour  = DataLoader(mnist_test, batch_size=64, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = SimpleMLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN-3hnhSMLA0",
        "outputId": "2e5bcf97-d6c1-4369-8a41-78a593437d15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:10<00:00, 908kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 58.0kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.04MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_score = optim.SGD(model.parameters(), lr=0.01)\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_score.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer_score.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "    print(f\"Epoch {epoch}: training loss = {running_loss / len(train_dataset)}\")\n",
        "\n",
        "# Save the optimized parameters\n",
        "optimized_params = [p.detach().clone() for p in model.parameters()]\n",
        "torch.save(model, 'only_opt_no_contour.pth')\n",
        "model.eval()\n",
        "running_loss = 0.0\n",
        "for data, target in train_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    running_loss += loss.item() * data.size(0)\n",
        "print(f\"Final training loss = {running_loss / len(train_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrDD8-IphWzL",
        "outputId": "69e6db65-75c3-4f1f-b67d-047ef177cc0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training loss = 1.2287294233004251\n",
            "Epoch 1: training loss = 0.4888212276538213\n",
            "Epoch 2: training loss = 0.38988907759984337\n",
            "Epoch 3: training loss = 0.3490614926099777\n",
            "Epoch 4: training loss = 0.3239420925617218\n",
            "Epoch 5: training loss = 0.3056267394463221\n",
            "Epoch 6: training loss = 0.2909077728827794\n",
            "Epoch 7: training loss = 0.27841718871593474\n",
            "Epoch 8: training loss = 0.26769361673196157\n",
            "Epoch 9: training loss = 0.2575781427224477\n",
            "Epoch 10: training loss = 0.24875897912979125\n",
            "Epoch 11: training loss = 0.24051359912157058\n",
            "Epoch 12: training loss = 0.23292791369756063\n",
            "Epoch 13: training loss = 0.22584411851565042\n",
            "Epoch 14: training loss = 0.219151235473156\n",
            "Epoch 15: training loss = 0.21286057914098103\n",
            "Epoch 16: training loss = 0.2067700740814209\n",
            "Epoch 17: training loss = 0.20116818931897482\n",
            "Epoch 18: training loss = 0.19573696269989013\n",
            "Epoch 19: training loss = 0.19063286929130555\n",
            "Final training loss = 0.18659229550759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('only_opt_no_contour.pth')\n",
        "# Set contour-walking hyperparameters:\n",
        "L = 2\n",
        "eta = 1e-5  # step size for contour walking\n",
        "T = int(L / eta)\n",
        "\n",
        "optimizer_contour = ContourWalkingOptimizer(model.parameters(), lr=eta)\n",
        "\n",
        "# Get one batch from the training loader:\n",
        "\n",
        "t = 0\n",
        "model.train()\n",
        "running_loss = 0\n",
        "while t < T:\n",
        "  for data, target in train_loader:\n",
        "      if t < T:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_contour.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer_contour.step()\n",
        "        running_loss += loss.item()\n",
        "        t+=1\n",
        "print(f\"Contour Step {t}: loss = {running_loss/ T}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YzkR-c6hUM1",
        "outputId": "37ff73b5-b334-4bae-8745-f67eb8d23bf6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-52cfae411de7>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('only_opt_no_contour.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contour Step 199999: loss = 0.1866070910475697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (p_orig, p_contour) in enumerate(zip(optimized_params, model.parameters())):\n",
        "    diff_norm = torch.norm(p_orig - p_contour)\n",
        "    print(f\"Parameter {i}: Norm difference = {diff_norm.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnRHQU13htuB",
        "outputId": "7b63dfb1-b73b-4553-8efe-8efe67863214"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter 0: Norm difference = 1.9969573020935059\n",
            "Parameter 1: Norm difference = 1.892081379890442\n",
            "Parameter 2: Norm difference = 1.9540207386016846\n",
            "Parameter 3: Norm difference = 1.997733473777771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check if it is o.k to walk on the contour of the validation set"
      ],
      "metadata": {
        "id": "xBSv1Fm5OuTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('only_opt_no_contour.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFEwTL-QO5fA",
        "outputId": "c7dc71ba-fca1-48f6-f59c-6fac469bbf28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-35ea222ebc2f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('only_opt_no_contour.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set contour-walking hyperparameters:\n",
        "L = 2\n",
        "eta = 1e-5  # step size for contour walking\n",
        "T = int(L / eta)\n",
        "\n",
        "optimizer_contour = ContourWalkingOptimizer(model.parameters(), lr=eta)\n",
        "t = 0\n",
        "#### The main difference here is that we walk on the contour relative to validation and not train #####\n",
        "T = int(1 + T / len(val_loader)) * len(val_loader)\n",
        "num_exp = 10\n",
        "valdation_contour = list()\n",
        "validation_no_contour = list()\n",
        "loader_for_contour = val_loader\n",
        "loader_for_no_contour = val_loader_no_contour\n",
        "for ind in range(num_exp):\n",
        "\n",
        "  running_loss = 0\n",
        "  model = torch.load('only_opt_no_contour.pth')\n",
        "  while t < T:\n",
        "    model.train()\n",
        "    for data, target in loader_for_contour:\n",
        "      if t < T:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          optimizer_contour.zero_grad()\n",
        "          output = model(data)\n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer_contour.step()\n",
        "          running_loss += loss.item()\n",
        "          t +=1\n",
        "  print(f\"Contour Step {t}: loss = {running_loss / T}\")\n",
        "  model.eval()\n",
        "  running_loss = 0\n",
        "  for data, target in loader_for_contour:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        with torch.no_grad():\n",
        "          output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        running_loss += loss.item()\n",
        "  print(f\"Validation loss = {running_loss/ len(loader_for_contour)}\")\n",
        "  valdation_contour.append(running_loss/ len(loader_for_contour))\n",
        "  model.eval()\n",
        "  running_loss = 0\n",
        "  for data, target in loader_for_no_contour:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        with torch.no_grad():\n",
        "          output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        running_loss += loss.item()\n",
        "  print(f\"Validation loss no contour = {running_loss/ len(loader_for_no_contour)}\")\n",
        "  validation_no_contour.append(running_loss/ len(loader_for_no_contour))\n",
        "  if loader_for_contour == val_loader:\n",
        "    loader_for_contour = val_loader_no_contour\n",
        "    loader_for_no_contour = val_loader\n",
        "  else:\n",
        "    loader_for_contour = val_loader\n",
        "    loader_for_no_contour = val_loader_no_contour\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crwtrUUsjfvO",
        "outputId": "a99b9b3c-de35-4e1d-a69f-4d86298b4cc7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3ffabda2b725>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('only_opt_no_contour.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contour Step 200018: loss = 0.190073547615198\n",
            "Validation loss = 0.19038610122386057\n",
            "Validation loss no contour = 0.18675488862357323\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.1867793531981623\n",
            "Validation loss no contour = 0.19009256547993156\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.18981870050263253\n",
            "Validation loss no contour = 0.18660990267422548\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.18686483881086302\n",
            "Validation loss no contour = 0.19052490535055755\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.18935113238870718\n",
            "Validation loss no contour = 0.18840125238724575\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.18822376821546039\n",
            "Validation loss no contour = 0.19089671530446428\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.19087751238209427\n",
            "Validation loss no contour = 0.18830052867626687\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.18666161879137821\n",
            "Validation loss no contour = 0.18965274059943332\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.19022793007219674\n",
            "Validation loss no contour = 0.18682211874776586\n",
            "Contour Step 200018: loss = 0.0\n",
            "Validation loss = 0.18710059378367322\n",
            "Validation loss no contour = 0.18927925094297737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Suppose you collected paired samples:\n",
        "A_v_before = valdation_contour  # validation accuracies before contour walk\n",
        "A_v_after = validation_no_contour   # validation accuracies after contour walk\n",
        "\n",
        "t_stat, p_value = ttest_rel(A_v_before, A_v_after)\n",
        "\n",
        "print(f\"Paired t-test result: t-stat={t_stat}, p-value={p_value}\")\n",
        "if p_value > 0.05:\n",
        "    print(\"Fail to reject H0: no significant bias from contour walk.\")\n",
        "else:\n",
        "    print(\"Reject H0: significant bias detected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0TG6PTVnfvc",
        "outputId": "ce630a33-250b-43f6-896d-89f4490a4751"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paired t-test result: t-stat=-0.10567469168607356, p-value=0.9181582809976772\n",
            "Fail to reject H0: no significant bias from contour walk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8sRle3Jb7806"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}